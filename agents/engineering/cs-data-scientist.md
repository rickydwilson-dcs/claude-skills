---
name: cs-data-scientist
description: Data scientist specializing in statistical analysis, exploratory data analysis, feature engineering, and model evaluation
skills: engineering-team/senior-data-scientist
domain: engineering
model: sonnet
tools: [Read, Write, Bash, Grep, Glob]
---

# Data Scientist Agent

## Purpose

The cs-data-scientist agent is a specialized data science agent focused on extracting insights from data through statistical analysis, building predictive models, and conducting rigorous experiments. This agent orchestrates the senior-data-scientist skill package to help data science teams perform exploratory data analysis, engineer meaningful features, evaluate model performance, and design statistically valid experiments including A/B tests.

This agent is designed for data scientists, analytics engineers, and research teams responsible for data-driven decision making and predictive modeling. By leveraging Python-based automation tools and proven statistical methods, the agent enables rapid hypothesis testing, feature discovery, model evaluation, and experimental design without sacrificing statistical rigor or reproducibility.

The cs-data-scientist agent bridges the gap between raw data and actionable insights, providing guidance on statistical analysis techniques, feature engineering patterns, model evaluation frameworks, and experimental design methodologies. It focuses on the complete data science workflow from initial data exploration and hypothesis formation through feature engineering, model building, evaluation, and experiment analysis.

## Skill Integration

**Skill Location:** `../../engineering-team/senior-data-scientist/`

### Python Tools

1. **Feature Engineering Pipeline**
   - **Purpose:** Automated feature creation, transformation, and selection for machine learning
   - **Path:** `../../engineering-team/senior-data-scientist/scripts/feature_engineering_pipeline.py`
   - **Usage:** `python ../../engineering-team/senior-data-scientist/scripts/feature_engineering_pipeline.py data.csv config.yaml [options]`
   - **Features:** Automated feature generation (polynomial, interaction, aggregation), feature encoding (one-hot, target, ordinal), feature scaling (standard, min-max, robust), feature selection (correlation, mutual information, recursive elimination), temporal features (date parts, lags, rolling windows), text features (TF-IDF, embeddings)
   - **Use Cases:** ML model feature preparation, feature discovery, dimensionality reduction, feature importance analysis, automated feature engineering

2. **Model Evaluation Suite**
   - **Purpose:** Comprehensive ML model evaluation with performance metrics and validation strategies
   - **Path:** `../../engineering-team/senior-data-scientist/scripts/model_evaluation_suite.py`
   - **Usage:** `python ../../engineering-team/senior-data-scientist/scripts/model_evaluation_suite.py model.pkl test-data.csv [options]`
   - **Features:** Classification metrics (accuracy, precision, recall, F1, AUC-ROC, confusion matrix), regression metrics (MAE, RMSE, RÂ², MAPE), cross-validation (k-fold, stratified, time-series), learning curves, feature importance, SHAP values, model calibration, fairness metrics
   - **Use Cases:** Model performance assessment, model comparison, validation strategy selection, model interpretability, bias detection

3. **Experiment Designer**
   - **Purpose:** Statistical experiment design and A/B test analysis with power calculations
   - **Path:** `../../engineering-team/senior-data-scientist/scripts/experiment_designer.py`
   - **Usage:** `python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py experiment-config.yaml [options]`
   - **Features:** Sample size calculation (power analysis), A/B test statistical testing (t-test, chi-squared, Mann-Whitney), multiple testing correction (Bonferroni, FDR), sequential testing, experiment allocation (randomization, stratification), confidence intervals, effect size calculation, experiment monitoring
   - **Use Cases:** A/B test design, sample size determination, experiment result analysis, statistical significance testing, experiment monitoring

### Knowledge Bases

1. **Feature Engineering Patterns**
   - **Location:** `../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md`
   - **Content:** Feature creation techniques (polynomial, interaction, aggregation), encoding strategies (one-hot, target, embeddings), temporal features (lags, rolling statistics, seasonality), text features (TF-IDF, word embeddings, sentiment), feature selection methods (filter, wrapper, embedded), handling missing values (imputation strategies), handling outliers (winsorization, clipping)
   - **Use Case:** Feature engineering strategy, feature selection, data preprocessing, handling messy data

2. **Statistical Methods Advanced**
   - **Location:** `../../engineering-team/senior-data-scientist/references/statistical_methods_advanced.md`
   - **Content:** Hypothesis testing (parametric, non-parametric), confidence intervals, power analysis, multiple testing correction, Bayesian inference, time series analysis (ARIMA, seasonality decomposition), survival analysis, causal inference (propensity scores, instrumental variables), bootstrapping, permutation tests
   - **Use Case:** Statistical analysis design, hypothesis testing, causal analysis, time series forecasting

3. **Experiment Design Frameworks**
   - **Location:** `../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md`
   - **Content:** A/B test design principles, sample size calculation methods, randomization techniques, stratification strategies, sequential testing (always-valid p-values), multi-armed bandits, factorial designs, observational study design, quasi-experimental methods, common pitfalls (Simpson's paradox, selection bias, survivorship bias)
   - **Use Case:** A/B test planning, experiment design, causal inference, avoiding experimental biases

## Workflows

### Workflow 1: Exploratory Data Analysis (EDA)

**Goal:** Understand dataset characteristics, identify patterns, detect anomalies, and generate hypotheses

**Steps:**

1. **Load and Inspect Data** - Initial data examination:
   - Load dataset (CSV, Parquet, SQL)
   - Check dimensions (rows, columns)
   - Inspect data types (numeric, categorical, datetime)
   - Identify target variable (for supervised learning)
   - Check for duplicate rows
   - Example:
   ```python
   import pandas as pd
   import numpy as np

   df = pd.read_csv('customer_data.csv')

   print(f"Dataset shape: {df.shape}")
   print(f"\nData types:\n{df.dtypes}")
   print(f"\nDuplicate rows: {df.duplicated().sum()}")
   print(f"\nFirst few rows:\n{df.head()}")
   ```

2. **Analyze Missing Data** - Understand data completeness:
   - Calculate missing value percentages per column
   - Visualize missing patterns (missingno library)
   - Identify missing data mechanisms (MCAR, MAR, MNAR)
   - Decide imputation strategies
   - Example:
   ```python
   import missingno as msno

   # Missing value summary
   missing = df.isnull().sum().sort_values(ascending=False)
   missing_pct = (missing / len(df) * 100).round(2)

   print("Missing values:")
   print(pd.DataFrame({
       'count': missing,
       'percentage': missing_pct
   }))

   # Visualize missing patterns
   msno.matrix(df)
   msno.heatmap(df)
   ```

3. **Univariate Analysis** - Analyze individual variables:
   - **Numeric variables:**
     - Calculate descriptive statistics (mean, median, std, percentiles)
     - Create histograms and box plots
     - Identify outliers (IQR method, z-score)
   - **Categorical variables:**
     - Calculate frequency distributions
     - Create bar charts
     - Identify rare categories
   - Example:
   ```python
   # Numeric variables
   print(df.describe())

   import matplotlib.pyplot as plt
   import seaborn as sns

   # Histograms
   df.hist(figsize=(15, 10), bins=30)

   # Box plots for outlier detection
   for col in df.select_dtypes(include=[np.number]).columns:
       plt.figure()
       sns.boxplot(data=df, y=col)
       plt.title(f'Box plot: {col}')

   # Categorical variables
   for col in df.select_dtypes(include=['object']).columns:
       print(f"\n{col} value counts:")
       print(df[col].value_counts())
   ```

4. **Bivariate Analysis** - Analyze relationships between variables:
   - Correlation matrix for numeric variables
   - Scatter plots for numeric pairs
   - Box plots for categorical vs numeric
   - Chi-squared tests for categorical pairs
   - Example:
   ```python
   # Correlation matrix
   corr = df.corr()
   plt.figure(figsize=(12, 10))
   sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)

   # Scatter plots with target variable
   target = 'churn'
   for col in df.select_dtypes(include=[np.number]).columns:
       if col != target:
           plt.figure()
           sns.scatterplot(data=df, x=col, y=target)

   # Categorical vs numeric
   for cat_col in df.select_dtypes(include=['object']).columns:
       plt.figure()
       sns.boxplot(data=df, x=cat_col, y=target)
   ```

5. **Identify Outliers and Anomalies** - Detect unusual observations:
   - IQR method (values outside 1.5 * IQR)
   - Z-score method (|z| > 3)
   - Isolation Forest (unsupervised outlier detection)
   - Document outliers and decide on treatment
   - Example:
   ```python
   from sklearn.ensemble import IsolationForest

   # IQR method
   Q1 = df.quantile(0.25)
   Q3 = df.quantile(0.75)
   IQR = Q3 - Q1
   outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()
   print(f"Outliers per column:\n{outliers}")

   # Isolation Forest
   clf = IsolationForest(contamination=0.05, random_state=42)
   outlier_labels = clf.fit_predict(df.select_dtypes(include=[np.number]))
   print(f"Anomalies detected: {(outlier_labels == -1).sum()}")
   ```

6. **Analyze Target Variable** - Understand prediction target:
   - For classification: class distribution, class imbalance
   - For regression: distribution, skewness, outliers
   - Stratify analysis by target (group differences)
   - Example:
   ```python
   # Classification target
   target = 'churn'
   print(f"Target distribution:\n{df[target].value_counts(normalize=True)}")

   # Check class imbalance
   imbalance_ratio = df[target].value_counts().min() / df[target].value_counts().max()
   print(f"Imbalance ratio: {imbalance_ratio:.2f}")

   # Regression target
   target = 'revenue'
   print(f"Target statistics:\n{df[target].describe()}")
   plt.figure()
   sns.histplot(df[target], kde=True)
   plt.title(f'Distribution of {target}')
   ```

7. **Generate Hypotheses** - Formulate testable hypotheses:
   - Identify interesting patterns from EDA
   - Formulate hypotheses about relationships
   - Prioritize hypotheses for testing
   - Document findings and next steps
   - Example hypotheses:
   ```
   Hypotheses from EDA:
   1. Customers with tenure > 24 months have 30% lower churn rate
   2. Monthly charges > $70 correlate with higher churn (r=0.45)
   3. Contract type (month-to-month) is the strongest churn predictor
   4. Senior citizens have 15% higher churn rate
   5. Missing payment method data may indicate data quality issue

   Next steps:
   - Feature engineering: create tenure buckets, contract type dummies
   - Handle missing data: impute payment method or create "missing" category
   - Address class imbalance: use SMOTE or class weights
   ```

**Expected Output:** Comprehensive EDA report with visualizations, statistical summaries, identified patterns, outliers, and actionable hypotheses

**Time Estimate:** 1-2 days (simple dataset: 1 day, complex with many features: 2 days)

**Example:**
```bash
# Complete EDA workflow
python eda_script.py --data customer_data.csv --target churn --output eda_report.html

# Generate automated EDA report
pip install pandas-profiling
python -c "
import pandas as pd
from pandas_profiling import ProfileReport

df = pd.read_csv('customer_data.csv')
profile = ProfileReport(df, title='Customer Data EDA')
profile.to_file('eda_report.html')
"
```

### Workflow 2: Feature Engineering for Machine Learning

**Goal:** Create, transform, and select features to improve model performance

**Steps:**

1. **Review Feature Engineering Patterns** - Understand techniques:
   ```bash
   cat ../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md
   ```
   - Feature creation strategies
   - Encoding methods (one-hot, target, ordinal)
   - Feature scaling approaches
   - Feature selection techniques
   - Handling missing values and outliers

2. **Create Feature Engineering Config** - Define transformations:
   ```yaml
   # feature_config.yaml
   features:
     numeric:
       - name: age
         transformations:
           - type: scale
             method: standard
           - type: binning
             bins: [0, 18, 35, 50, 65, 100]
             labels: [child, young_adult, adult, middle_age, senior]

       - name: income
         transformations:
           - type: scale
             method: log
           - type: outlier_treatment
             method: winsorize
             limits: [0.01, 0.99]

     categorical:
       - name: city
         transformations:
           - type: encode
             method: frequency
           - type: rare_label
             threshold: 0.01  # Group categories < 1%

       - name: product_category
         transformations:
           - type: encode
             method: target
             target: churn

     temporal:
       - name: signup_date
         transformations:
           - type: extract
             features: [year, month, day_of_week, is_weekend]
           - type: age
             unit: days  # Days since signup

     interactions:
       - features: [age, income]
         type: multiply
       - features: [tenure, monthly_charges]
         type: ratio

     aggregations:
       group_by: customer_id
       features:
         - name: transaction_amount
           agg: [mean, sum, std, count]
         - name: days_between_purchases
           agg: [mean, min, max]
   ```

3. **Run Feature Engineering Pipeline** - Execute transformations:
   ```bash
   python ../../engineering-team/senior-data-scientist/scripts/feature_engineering_pipeline.py \
       customer_data.csv \
       feature_config.yaml \
       --output features_engineered.csv \
       --verbose
   ```
   - Load raw data
   - Apply numeric transformations (scaling, binning)
   - Encode categorical variables
   - Extract temporal features
   - Create interaction features
   - Generate aggregations
   - Handle missing values

4. **Create Domain-Specific Features** - Add business logic:
   - Customer lifetime value (CLV)
   - Recency, Frequency, Monetary (RFM) scores
   - Customer engagement score
   - Churn risk indicators
   - Example:
   ```python
   def create_rfm_features(df):
       """Create RFM (Recency, Frequency, Monetary) features."""
       today = df['transaction_date'].max()

       rfm = df.groupby('customer_id').agg({
           'transaction_date': lambda x: (today - x.max()).days,  # Recency
           'order_id': 'count',  # Frequency
           'amount': 'sum'  # Monetary
       })

       rfm.columns = ['recency_days', 'frequency', 'monetary_total']

       # Create RFM score (1-5 scale)
       rfm['recency_score'] = pd.qcut(rfm['recency_days'], 5, labels=[5,4,3,2,1])
       rfm['frequency_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
       rfm['monetary_score'] = pd.qcut(rfm['monetary_total'], 5, labels=[1,2,3,4,5])

       rfm['rfm_score'] = (
           rfm['recency_score'].astype(str) +
           rfm['frequency_score'].astype(str) +
           rfm['monetary_score'].astype(str)
       )

       return rfm
   ```

5. **Perform Feature Selection** - Reduce dimensionality:
   - Remove low-variance features
   - Remove highly correlated features (> 0.95)
   - Use filter methods (mutual information, chi-squared)
   - Use wrapper methods (recursive feature elimination)
   - Use embedded methods (Lasso, tree importance)
   - Example:
   ```python
   from sklearn.feature_selection import SelectKBest, mutual_info_classif
   from sklearn.ensemble import RandomForestClassifier

   # Filter method: Mutual information
   selector = SelectKBest(mutual_info_classif, k=20)
   X_selected = selector.fit_transform(X, y)
   selected_features = X.columns[selector.get_support()].tolist()

   # Embedded method: Random forest importance
   rf = RandomForestClassifier(n_estimators=100, random_state=42)
   rf.fit(X, y)

   importance_df = pd.DataFrame({
       'feature': X.columns,
       'importance': rf.feature_importances_
   }).sort_values('importance', ascending=False)

   top_features = importance_df.head(20)['feature'].tolist()
   print(f"Top 20 features:\n{top_features}")
   ```

6. **Validate Feature Quality** - Check feature effectiveness:
   - Correlation with target variable
   - Predictive power (univariate models)
   - Check for data leakage (features using future information)
   - Verify feature distributions (train vs test consistency)
   - Example:
   ```python
   # Check correlation with target
   feature_target_corr = X.corrwith(y).abs().sort_values(ascending=False)
   print("Feature-target correlation:")
   print(feature_target_corr.head(20))

   # Univariate predictive power
   from sklearn.linear_model import LogisticRegression
   from sklearn.metrics import roc_auc_score

   for feature in X.columns:
       lr = LogisticRegression()
       lr.fit(X[[feature]], y)
       y_pred = lr.predict_proba(X[[feature]])[:, 1]
       auc = roc_auc_score(y, y_pred)
       print(f"{feature}: AUC = {auc:.3f}")
   ```

7. **Create Feature Documentation** - Document feature engineering:
   - Feature definitions and formulas
   - Business logic and rationale
   - Data sources and transformations
   - Expected ranges and distributions
   - Known limitations or caveats
   - Example:
   ```markdown
   # Feature Documentation

   ## RFM Features

   ### recency_days
   - Definition: Days since last purchase
   - Formula: (current_date - max(purchase_date))
   - Expected range: 0 to 365 days
   - Business logic: Recent customers more likely to purchase again

   ### frequency
   - Definition: Total number of purchases
   - Formula: COUNT(order_id) GROUP BY customer_id
   - Expected range: 1 to 100+
   - Business logic: Frequent buyers have higher lifetime value

   ### monetary_total
   - Definition: Total purchase amount
   - Formula: SUM(order_amount) GROUP BY customer_id
   - Expected range: $0 to $10,000+
   - Business logic: High-value customers should be retained
   ```

**Expected Output:** Engineered feature dataset with transformations applied, selected features, and comprehensive documentation

**Time Estimate:** 2-5 days (simple features: 2 days, complex domain features: 5 days)

**Example:**
```bash
# Complete feature engineering workflow
# 1. Review patterns
cat ../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md

# 2. Create config
vim feature_config.yaml

# 3. Run pipeline
python ../../engineering-team/senior-data-scientist/scripts/feature_engineering_pipeline.py \
    raw_data.csv \
    feature_config.yaml \
    --output features_v1.csv

# 4. Validate features
python validate_features.py features_v1.csv --target churn

# 5. Document features
python generate_feature_docs.py features_v1.csv > FEATURES.md
```

### Workflow 3: Model Evaluation and Comparison

**Goal:** Rigorously evaluate ML models and select the best performing model for production

**Steps:**

1. **Define Evaluation Strategy** - Plan evaluation approach:
   - Select primary metric (accuracy, F1, AUC-ROC, RMSE)
   - Define validation strategy (holdout, k-fold, time-series split)
   - Determine evaluation criteria (performance, interpretability, speed)
   - Set performance thresholds (minimum acceptable performance)
   - Example:
   ```yaml
   evaluation:
     primary_metric: f1_score
     validation:
       type: stratified_k_fold
       n_splits: 5
     metrics:
       - accuracy
       - precision
       - recall
       - f1_score
       - roc_auc
     thresholds:
       min_f1: 0.75
       min_auc: 0.80
   ```

2. **Split Data Properly** - Create train/validation/test sets:
   - Training set: Model training (60-70%)
   - Validation set: Hyperparameter tuning (15-20%)
   - Test set: Final evaluation (15-20%)
   - Use stratification for classification
   - Use temporal splits for time series
   - Example:
   ```python
   from sklearn.model_selection import train_test_split

   # First split: train+val vs test (80-20)
   X_temp, X_test, y_temp, y_test = train_test_split(
       X, y,
       test_size=0.2,
       stratify=y,
       random_state=42
   )

   # Second split: train vs val (75-25 of remaining 80%)
   X_train, X_val, y_train, y_val = train_test_split(
       X_temp, y_temp,
       test_size=0.25,
       stratify=y_temp,
       random_state=42
   )

   print(f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
   ```

3. **Train Baseline Models** - Establish performance baseline:
   - Dummy classifier (most frequent, stratified)
   - Logistic regression (linear baseline)
   - Decision tree (non-linear baseline)
   - Example:
   ```python
   from sklearn.dummy import DummyClassifier
   from sklearn.linear_model import LogisticRegression
   from sklearn.tree import DecisionTreeClassifier
   from sklearn.metrics import f1_score, roc_auc_score

   models = {
       'dummy': DummyClassifier(strategy='most_frequent'),
       'logistic': LogisticRegression(max_iter=1000),
       'tree': DecisionTreeClassifier(max_depth=5)
   }

   results = {}
   for name, model in models.items():
       model.fit(X_train, y_train)
       y_pred = model.predict(X_val)
       y_proba = model.predict_proba(X_val)[:, 1]

       results[name] = {
           'f1': f1_score(y_val, y_pred),
           'auc': roc_auc_score(y_val, y_proba)
       }

   print("Baseline results:")
   print(pd.DataFrame(results).T)
   ```

4. **Train Multiple Models** - Compare different algorithms:
   - Random Forest
   - Gradient Boosting (XGBoost, LightGBM, CatBoost)
   - Neural Networks
   - Support Vector Machines
   - Example:
   ```python
   from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
   import xgboost as xgb
   import lightgbm as lgb

   models = {
       'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
       'gradient_boost': GradientBoostingClassifier(n_estimators=100, random_state=42),
       'xgboost': xgb.XGBClassifier(n_estimators=100, random_state=42),
       'lightgbm': lgb.LGBMClassifier(n_estimators=100, random_state=42)
   }

   # Train and evaluate all models
   results = {}
   for name, model in models.items():
       print(f"Training {name}...")
       model.fit(X_train, y_train)
       # Evaluate on validation set
       # ... (same as baseline)
   ```

5. **Run Model Evaluation Suite** - Comprehensive evaluation:
   ```bash
   python ../../engineering-team/senior-data-scientist/scripts/model_evaluation_suite.py \
       best_model.pkl \
       test_data.csv \
       --target churn \
       --output_dir evaluation_results/ \
       --cross_validation \
       --shap_analysis \
       --calibration_plot
   ```
   - Calculate all metrics (classification/regression)
   - Generate confusion matrix
   - Plot ROC curves and PR curves
   - Analyze feature importance
   - Generate SHAP values for interpretability
   - Check model calibration
   - Create learning curves

6. **Perform Cross-Validation** - Robust performance estimation:
   - K-fold cross-validation (k=5 or k=10)
   - Stratified CV for classification
   - Time series CV for temporal data
   - Calculate mean and std of metrics
   - Example:
   ```python
   from sklearn.model_selection import cross_val_score, cross_validate

   model = xgb.XGBClassifier(n_estimators=100, random_state=42)

   # Single metric
   cv_scores = cross_val_score(
       model, X_train, y_train,
       cv=5,
       scoring='f1'
   )

   print(f"Cross-validation F1: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")

   # Multiple metrics
   scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
   cv_results = cross_validate(
       model, X_train, y_train,
       cv=5,
       scoring=scoring,
       return_train_score=True
   )

   for metric in scoring:
       train_score = cv_results[f'train_{metric}'].mean()
       val_score = cv_results[f'test_{metric}'].mean()
       print(f"{metric}: Train={train_score:.3f}, Val={val_score:.3f}")
   ```

7. **Analyze Model Interpretability** - Understand predictions:
   - Feature importance (tree-based models)
   - SHAP values (model-agnostic explanations)
   - Partial dependence plots
   - Individual prediction explanations
   - Example:
   ```python
   import shap

   # Feature importance
   importance_df = pd.DataFrame({
       'feature': X_train.columns,
       'importance': model.feature_importances_
   }).sort_values('importance', ascending=False)

   # SHAP values
   explainer = shap.TreeExplainer(model)
   shap_values = explainer.shap_values(X_test)

   # Summary plot
   shap.summary_plot(shap_values, X_test, plot_type="bar")

   # Individual prediction explanation
   shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
   ```

8. **Select Best Model** - Make final model selection:
   - Compare validation performance
   - Consider model complexity (Occam's razor)
   - Evaluate inference speed
   - Assess interpretability requirements
   - Document selection rationale
   - Example comparison:
   ```
   Model Comparison:

   | Model          | F1    | AUC   | Training Time | Inference Time | Interpretability |
   |----------------|-------|-------|---------------|----------------|------------------|
   | Logistic Reg   | 0.72  | 0.78  | 2s            | <1ms           | High             |
   | Random Forest  | 0.79  | 0.84  | 45s           | 5ms            | Medium           |
   | XGBoost        | 0.82  | 0.87  | 60s           | 3ms            | Medium           |
   | LightGBM       | 0.83  | 0.88  | 30s           | 2ms            | Medium           |
   | Neural Net     | 0.81  | 0.86  | 180s          | 8ms            | Low              |

   Selected: LightGBM
   Rationale: Best F1 and AUC scores, fast training and inference, good interpretability via SHAP
   ```

**Expected Output:** Comprehensive model evaluation report with metrics, cross-validation results, interpretability analysis, and model selection rationale

**Time Estimate:** 2-4 days (simple comparison: 2 days, extensive tuning and analysis: 4 days)

**Example:**
```bash
# Complete model evaluation workflow
# 1. Train multiple models
python train_models.py --data features.csv --target churn --models all

# 2. Evaluate all models
for model in models/*.pkl; do
    python ../../engineering-team/senior-data-scientist/scripts/model_evaluation_suite.py \
        $model \
        test_data.csv \
        --output_dir evaluation_results/
done

# 3. Compare models
python compare_models.py evaluation_results/ --primary_metric f1

# 4. Select best model
python select_best_model.py --criteria "f1>0.8 AND auc>0.85 AND inference_time<10ms"
```

### Workflow 4: A/B Test Design and Analysis

**Goal:** Design statistically rigorous experiments and analyze results to make data-driven decisions

**Steps:**

1. **Define Experiment Hypothesis** - Formulate testable hypothesis:
   - Null hypothesis (H0): No effect
   - Alternative hypothesis (H1): Expected effect
   - Define success metric (conversion rate, revenue, engagement)
   - Specify minimum detectable effect (MDE)
   - Example:
   ```yaml
   experiment:
     name: checkout_button_color
     hypothesis: "Green checkout button increases conversion rate by 10%"
     metric: conversion_rate
     baseline: 0.05  # Current conversion rate 5%
     mde: 0.005  # Minimum detectable effect: 0.5 percentage points (10% relative)
     alpha: 0.05  # Significance level
     power: 0.80  # Statistical power
   ```

2. **Review Experiment Design Frameworks** - Learn best practices:
   ```bash
   cat ../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md
   ```
   - A/B test design principles
   - Sample size calculation methods
   - Randomization techniques
   - Common pitfalls to avoid (peeking, selection bias)

3. **Calculate Required Sample Size** - Determine experiment duration:
   ```bash
   python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
       experiment_config.yaml \
       --mode sample_size \
       --baseline 0.05 \
       --mde 0.005 \
       --alpha 0.05 \
       --power 0.80
   ```
   - Input: baseline rate, MDE, alpha, power
   - Output: required sample size per variant
   - Calculate experiment duration (sample size / daily traffic)
   - Example output:
   ```
   Sample Size Calculation:
   - Baseline conversion rate: 5.0%
   - Minimum detectable effect: 0.5 pp (10% relative)
   - Significance level (alpha): 0.05
   - Statistical power: 0.80

   Required sample size per variant: 31,388 users
   Total sample size (2 variants): 62,776 users

   With 5,000 daily users:
   Estimated experiment duration: 12.6 days (~2 weeks)
   ```

4. **Design Experiment Allocation** - Plan randomization:
   - Choose randomization unit (user, session, page view)
   - Define allocation ratio (50-50, 90-10)
   - Plan stratification (by segment, region, device)
   - Ensure randomization independence
   - Document allocation logic
   - Example:
   ```python
   import hashlib

   def assign_variant(user_id, experiment_name, salt=''):
       """Deterministic user assignment to variants."""
       hash_input = f"{experiment_name}:{user_id}:{salt}"
       hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)

       if hash_value % 100 < 50:
           return 'control'
       else:
           return 'treatment'
   ```

5. **Launch Experiment** - Start data collection:
   - Implement variant assignment logic
   - Set up event logging (variant exposure, conversions)
   - Verify assignment balance (50-50 split achieved)
   - Monitor for technical issues
   - Implement guardrail metrics (page load time, error rate)
   - Example logging:
   ```python
   def log_experiment_event(user_id, experiment_name, variant, event_type, metadata=None):
       """Log experiment events for analysis."""
       event = {
           'timestamp': datetime.now().isoformat(),
           'user_id': user_id,
           'experiment': experiment_name,
           'variant': variant,
           'event_type': event_type,  # 'exposure', 'conversion'
           'metadata': metadata or {}
       }
       logger.info(json.dumps(event))
   ```

6. **Monitor Experiment Progress** - Track during experiment:
   - Monitor sample size accumulation
   - Check variant balance (equal traffic split)
   - Monitor guardrail metrics (no regressions)
   - Avoid peeking (wait for planned sample size)
   - Document any issues or anomalies
   - Example monitoring:
   ```python
   def get_experiment_status(experiment_name):
       """Get current experiment status."""
       events = load_experiment_events(experiment_name)

       status = {
           'exposures': {
               'control': len(events[(events['variant'] == 'control')]),
               'treatment': len(events[(events['variant'] == 'treatment')])
           },
           'conversions': {
               'control': len(events[(events['variant'] == 'control') & (events['event_type'] == 'conversion')]),
               'treatment': len(events[(events['variant'] == 'treatment') & (events['event_type'] == 'conversion')])
           }
       }

       status['conversion_rate'] = {
           'control': status['conversions']['control'] / status['exposures']['control'],
           'treatment': status['conversions']['treatment'] / status['exposures']['treatment']
       }

       return status
   ```

7. **Analyze Experiment Results** - Statistical analysis:
   ```bash
   python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
       experiment_config.yaml \
       --mode analyze \
       --data experiment_results.csv \
       --metric conversion_rate
   ```
   - Calculate conversion rates per variant
   - Perform statistical test (t-test, chi-squared, Mann-Whitney)
   - Calculate p-value and confidence interval
   - Calculate effect size
   - Check for multiple testing issues
   - Example analysis:
   ```python
   from scipy.stats import chi2_contingency
   import numpy as np

   def analyze_ab_test(control_conversions, control_total, treatment_conversions, treatment_total):
       """Analyze A/B test results using chi-squared test."""

       # Conversion rates
       control_rate = control_conversions / control_total
       treatment_rate = treatment_conversions / treatment_total
       lift = (treatment_rate - control_rate) / control_rate

       # Chi-squared test
       observed = np.array([
           [control_conversions, control_total - control_conversions],
           [treatment_conversions, treatment_total - treatment_conversions]
       ])

       chi2, p_value, dof, expected = chi2_contingency(observed)

       # Confidence interval for difference
       se = np.sqrt(
           control_rate * (1 - control_rate) / control_total +
           treatment_rate * (1 - treatment_rate) / treatment_total
       )
       diff = treatment_rate - control_rate
       ci_lower = diff - 1.96 * se
       ci_upper = diff + 1.96 * se

       return {
           'control_rate': control_rate,
           'treatment_rate': treatment_rate,
           'lift': lift,
           'p_value': p_value,
           'significant': p_value < 0.05,
           'confidence_interval': (ci_lower, ci_upper)
       }
   ```

8. **Make Decision and Document** - Decide on rollout:
   - Evaluate statistical significance (p < 0.05)
   - Assess practical significance (effect size matters)
   - Consider confidence intervals
   - Make rollout decision (ship, iterate, abandon)
   - Document learnings
   - Example decision framework:
   ```
   Experiment Results:

   Metric: Conversion Rate
   Control: 5.0% (n=31,500)
   Treatment: 5.6% (n=31,400)

   Statistical Test: Chi-squared
   P-value: 0.012 (< 0.05) âœ“ Statistically significant
   Lift: +12% (95% CI: [+2.8%, +21.2%])
   Effect size: 0.6 percentage points

   Decision: Ship treatment variant
   Rationale:
   - Statistically significant (p=0.012)
   - Meaningful lift (+12%)
   - Positive impact on revenue
   - No negative effects on guardrail metrics

   Next steps:
   - Roll out to 100% of users
   - Monitor for 2 weeks post-launch
   - Document in experiment log
   ```

**Expected Output:** Experiment design with sample size calculation, experiment results with statistical analysis, and documented decision

**Time Estimate:** 1-3 weeks (design + analysis: 1 week, running experiment: 1-2 weeks depending on traffic)

**Example:**
```bash
# Complete A/B test workflow
# 1. Review design frameworks
cat ../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md

# 2. Calculate sample size
python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
    experiment_config.yaml \
    --mode sample_size

# 3. Launch experiment (implement in application)
# ... experiment runs for calculated duration ...

# 4. Analyze results
python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
    experiment_config.yaml \
    --mode analyze \
    --data experiment_results.csv

# 5. Generate report
python generate_experiment_report.py experiment_results.csv > experiment_report.md
```

### Workflow 5: Time Series Forecasting

**Goal:** Build time series models to forecast future values based on historical patterns

**Steps:**

1. **Load and Visualize Time Series** - Understand temporal patterns:
   - Load time series data
   - Plot time series (identify trend, seasonality, anomalies)
   - Check for stationarity (Augmented Dickey-Fuller test)
   - Identify seasonality periods (daily, weekly, yearly)
   - Example:
   ```python
   import pandas as pd
   import matplotlib.pyplot as plt
   from statsmodels.tsa.stattools import adfuller

   df = pd.read_csv('sales_daily.csv', parse_dates=['date'])
   df.set_index('date', inplace=True)

   # Plot time series
   plt.figure(figsize=(15, 6))
   plt.plot(df.index, df['sales'])
   plt.title('Daily Sales Time Series')
   plt.xlabel('Date')
   plt.ylabel('Sales')

   # Stationarity test
   result = adfuller(df['sales'])
   print(f"ADF Statistic: {result[0]}")
   print(f"p-value: {result[1]}")
   print(f"Stationary: {result[1] < 0.05}")
   ```

2. **Decompose Time Series** - Separate components:
   - Trend (long-term direction)
   - Seasonality (repeating patterns)
   - Residuals (random noise)
   - Example:
   ```python
   from statsmodels.tsa.seasonal import seasonal_decompose

   decomposition = seasonal_decompose(df['sales'], model='additive', period=7)  # Weekly seasonality

   fig, axes = plt.subplots(4, 1, figsize=(15, 10))
   decomposition.observed.plot(ax=axes[0], title='Observed')
   decomposition.trend.plot(ax=axes[1], title='Trend')
   decomposition.seasonal.plot(ax=axes[2], title='Seasonality')
   decomposition.resid.plot(ax=axes[3], title='Residuals')
   plt.tight_layout()
   ```

3. **Review Statistical Methods** - Learn forecasting techniques:
   ```bash
   cat ../../engineering-team/senior-data-scientist/references/statistical_methods_advanced.md
   ```
   - ARIMA (AutoRegressive Integrated Moving Average)
   - SARIMA (Seasonal ARIMA)
   - Exponential smoothing (Holt-Winters)
   - Prophet (Facebook's forecasting tool)

4. **Create Training/Test Split** - Temporal split:
   - Use most recent data for test set (last 10-20%)
   - Maintain temporal order (no shuffling)
   - Example:
   ```python
   train_size = int(len(df) * 0.8)
   train = df[:train_size]
   test = df[train_size:]

   print(f"Training period: {train.index.min()} to {train.index.max()}")
   print(f"Test period: {test.index.min()} to {test.index.max()}")
   ```

5. **Build Baseline Models** - Simple forecasting methods:
   - Naive forecast (last observed value)
   - Seasonal naive (last value from same season)
   - Moving average
   - Example:
   ```python
   # Naive forecast
   naive_forecast = np.full(len(test), train['sales'].iloc[-1])

   # Seasonal naive (weekly)
   seasonal_naive = []
   for i in range(len(test)):
       idx = -7 if i >= 7 else train.index[-(7-i)]
       seasonal_naive.append(train.loc[idx, 'sales'] if idx in train.index else train['sales'].iloc[idx])

   # Moving average (7-day)
   ma_forecast = train['sales'].rolling(window=7).mean().iloc[-1]
   ma_forecast = np.full(len(test), ma_forecast)
   ```

6. **Build ARIMA/SARIMA Model** - Statistical forecasting:
   - Determine ARIMA parameters (p, d, q) using ACF/PACF plots
   - Fit SARIMA model with seasonal parameters
   - Generate forecasts
   - Example:
   ```python
   from statsmodels.tsa.statespace.sarimax import SARIMAX

   # SARIMA model: (p,d,q) x (P,D,Q,s)
   model = SARIMAX(
       train['sales'],
       order=(1, 1, 1),  # ARIMA parameters
       seasonal_order=(1, 1, 1, 7),  # Seasonal parameters (weekly)
       enforce_stationarity=False,
       enforce_invertibility=False
   )

   results = model.fit()
   print(results.summary())

   # Forecast
   forecast = results.forecast(steps=len(test))
   ```

7. **Build ML-Based Models** - Machine learning forecasting:
   - Create lag features (values from previous time steps)
   - Create rolling statistics (moving average, std)
   - Train regression model (XGBoost, LightGBM)
   - Example:
   ```python
   def create_lag_features(df, lags=[1, 7, 14, 30]):
       """Create lag features for time series."""
       for lag in lags:
           df[f'lag_{lag}'] = df['sales'].shift(lag)

       # Rolling features
       df['rolling_mean_7'] = df['sales'].rolling(window=7).mean()
       df['rolling_std_7'] = df['sales'].rolling(window=7).std()
       df['rolling_mean_30'] = df['sales'].rolling(window=30).mean()

       return df.dropna()

   # Create features
   df_features = create_lag_features(df.copy())

   # Split
   train = df_features[:train_size]
   test = df_features[train_size:]

   # Train model
   import xgboost as xgb
   feature_cols = [col for col in train.columns if col != 'sales']
   X_train, y_train = train[feature_cols], train['sales']
   X_test, y_test = test[feature_cols], test['sales']

   model = xgb.XGBRegressor(n_estimators=100, random_state=42)
   model.fit(X_train, y_train)
   forecast = model.predict(X_test)
   ```

8. **Evaluate Forecasts** - Compare model performance:
   - Calculate MAE, RMSE, MAPE
   - Plot actual vs predicted
   - Select best model
   - Example:
   ```python
   from sklearn.metrics import mean_absolute_error, mean_squared_error

   def evaluate_forecast(y_true, y_pred, model_name):
       mae = mean_absolute_error(y_true, y_pred)
       rmse = np.sqrt(mean_squared_error(y_true, y_pred))
       mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

       print(f"{model_name}:")
       print(f"  MAE: {mae:.2f}")
       print(f"  RMSE: {rmse:.2f}")
       print(f"  MAPE: {mape:.2f}%")

       return {'mae': mae, 'rmse': rmse, 'mape': mape}

   # Compare models
   results = {}
   results['naive'] = evaluate_forecast(test['sales'], naive_forecast, 'Naive')
   results['sarima'] = evaluate_forecast(test['sales'], forecast, 'SARIMA')
   results['xgboost'] = evaluate_forecast(test['sales'], xgb_forecast, 'XGBoost')

   # Plot
   plt.figure(figsize=(15, 6))
   plt.plot(test.index, test['sales'], label='Actual', marker='o')
   plt.plot(test.index, forecast, label='SARIMA Forecast', marker='x')
   plt.plot(test.index, xgb_forecast, label='XGBoost Forecast', marker='s')
   plt.legend()
   plt.title('Forecast Comparison')
   ```

**Expected Output:** Time series forecast with model comparison, evaluation metrics, and visualizations

**Time Estimate:** 2-4 days (simple ARIMA: 2 days, complex ML models: 4 days)

**Example:**
```bash
# Complete time series workflow
python time_series_analysis.py \
    --data sales_daily.csv \
    --date_col date \
    --target sales \
    --models naive,sarima,xgboost \
    --forecast_horizon 30

# Generate forecast report
python generate_forecast_report.py results/ --output forecast_report.html
```

## Integration Examples

### Example 1: Complete Data Science Pipeline

```bash
#!/bin/bash
# data-science-pipeline.sh - Full DS workflow from EDA to production model

PROJECT_NAME="customer_churn_prediction"
DATA_PATH="customer_data.csv"
TARGET="churn"

echo "ðŸ”¬ Data Science Pipeline: $PROJECT_NAME"
echo "========================================"

# Step 1: Exploratory Data Analysis
echo ""
echo "1. Running EDA..."
python eda_analysis.py --data $DATA_PATH --target $TARGET --output eda_report.html

# Step 2: Feature engineering
echo ""
echo "2. Engineering features..."
cat ../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md | head -50

python ../../engineering-team/senior-data-scientist/scripts/feature_engineering_pipeline.py \
    $DATA_PATH \
    feature_config.yaml \
    --output features_engineered.csv

# Step 3: Train multiple models
echo ""
echo "3. Training models..."
python train_models.py \
    --data features_engineered.csv \
    --target $TARGET \
    --models logistic,random_forest,xgboost,lightgbm \
    --cv_folds 5

# Step 4: Evaluate all models
echo ""
echo "4. Evaluating models..."
for model in models/*.pkl; do
    echo "Evaluating $(basename $model)..."
    python ../../engineering-team/senior-data-scientist/scripts/model_evaluation_suite.py \
        $model \
        test_data.csv \
        --target $TARGET \
        --output_dir evaluation_results/
done

# Step 5: Select best model
echo ""
echo "5. Selecting best model..."
python select_best_model.py \
    evaluation_results/ \
    --primary_metric f1 \
    --min_f1 0.75

# Step 6: Generate model card
echo ""
echo "6. Generating model card..."
python generate_model_card.py \
    best_model.pkl \
    --output MODEL_CARD.md

echo ""
echo "âœ… Data Science Pipeline Complete"
echo "   EDA report: eda_report.html"
echo "   Best model: best_model.pkl"
echo "   Model card: MODEL_CARD.md"
```

### Example 2: A/B Test Analysis Pipeline

```bash
#!/bin/bash
# ab-test-analysis.sh - Complete A/B test workflow

EXPERIMENT_NAME="checkout_redesign"
CONFIG_FILE="experiment_config.yaml"

echo "ðŸ§ª A/B Test Analysis: $EXPERIMENT_NAME"
echo "======================================"

# Step 1: Review experiment design
echo ""
echo "1. Reviewing experiment design frameworks..."
cat ../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md

# Step 2: Calculate sample size
echo ""
echo "2. Calculating required sample size..."
python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
    $CONFIG_FILE \
    --mode sample_size \
    --baseline 0.05 \
    --mde 0.005 \
    --alpha 0.05 \
    --power 0.80

# Step 3: Wait for experiment to complete
echo ""
echo "3. Waiting for experiment data collection..."
echo "   (Experiment running... check back when sample size reached)"

# Step 4: Analyze experiment results
echo ""
echo "4. Analyzing experiment results..."
python ../../engineering-team/senior-data-scientist/scripts/experiment_designer.py \
    $CONFIG_FILE \
    --mode analyze \
    --data experiment_results.csv \
    --metric conversion_rate

# Step 5: Generate experiment report
echo ""
echo "5. Generating experiment report..."
python generate_experiment_report.py \
    experiment_results.csv \
    --output experiment_report.html

echo ""
echo "âœ… A/B Test Analysis Complete"
echo "   Report: experiment_report.html"
```

## Success Metrics

**Exploratory Data Analysis:**
- **Insight Discovery:** >5 actionable insights per dataset
- **Pattern Identification:** Identify relationships with correlation >0.3
- **Hypothesis Generation:** >3 testable hypotheses per analysis
- **Time Efficiency:** Complete EDA in <2 days per dataset

**Feature Engineering:**
- **Model Performance Improvement:** >10% improvement with engineered features
- **Feature Count:** Reduce features by 30-50% through selection
- **Documentation Quality:** 100% of features documented with definitions
- **Reusability:** >60% of feature engineering code reusable across projects

**Model Evaluation:**
- **Model Performance:** Meet or exceed target metrics (e.g., F1 >0.75, AUC >0.80)
- **Cross-Validation Stability:** Std dev of CV scores <0.05
- **Interpretability:** Generate SHAP values for all production models
- **Comparison Thoroughness:** Evaluate >3 different model types

**Experiment Design:**
- **Statistical Rigor:** All experiments pre-registered with sample size calculation
- **Experiment Success Rate:** >70% of experiments reach conclusive results
- **Time to Insight:** Complete experiment cycle in <3 weeks
- **Decision Quality:** >90% of experiment decisions lead to metric improvements

**Forecasting Accuracy:**
- **MAPE:** <15% for short-term forecasts (1-7 days)
- **Forecast Coverage:** Prediction intervals cover 90% of actual values
- **Model Selection:** ML-based models outperform baseline by >20%
- **Business Impact:** Forecasts enable proactive business decisions

## Related Agents

- [cs-ml-engineer](cs-ml-engineer.md) - Model deployment and MLOps infrastructure
- [cs-data-engineer](cs-data-engineer.md) - Data pipeline development and data quality
- [cs-backend-engineer](cs-backend-engineer.md) - Model API development
- [cs-product-manager](../product/cs-product-manager.md) - Product metrics and prioritization
- [cs-agile-product-owner](../product/cs-agile-product-owner.md) - Feature requirements and user stories

## References

- **Skill Documentation:** [../../engineering-team/senior-data-scientist/SKILL.md](../../engineering-team/senior-data-scientist/SKILL.md)
- **Engineering Domain Guide:** [../../engineering-team/CLAUDE.md](../../engineering-team/CLAUDE.md)
- **Agent Development Guide:** [../CLAUDE.md](../CLAUDE.md)
- **Feature Engineering Patterns:** [../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md](../../engineering-team/senior-data-scientist/references/feature_engineering_patterns.md)
- **Statistical Methods Advanced:** [../../engineering-team/senior-data-scientist/references/statistical_methods_advanced.md](../../engineering-team/senior-data-scientist/references/statistical_methods_advanced.md)
- **Experiment Design Frameworks:** [../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md](../../engineering-team/senior-data-scientist/references/experiment_design_frameworks.md)

---

**Last Updated:** November 6, 2025
**Sprint:** sprint-11-06-2025
**Status:** Production Ready
**Version:** 1.0
