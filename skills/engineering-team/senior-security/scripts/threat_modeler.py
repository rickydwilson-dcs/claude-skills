#!/usr/bin/env python3
"""
Threat Modeler - STRIDE Security Threat Analysis Tool

Automated threat modeling tool that performs:
- STRIDE threat categorization (Spoofing, Tampering, Repudiation, Info Disclosure, DoS, Elevation)
- Attack surface mapping and analysis
- Threat identification and prioritization
- Mitigation recommendations and control mapping

Part of the senior-security skill package.
"""

import argparse
import csv
import json
import logging
import os
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from io import StringIO
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ThreatCategory(Enum):
    """STRIDE threat categories"""
    SPOOFING = "Spoofing Identity"
    TAMPERING = "Tampering with Data"
    REPUDIATION = "Repudiation"
    INFORMATION_DISCLOSURE = "Information Disclosure"
    DENIAL_OF_SERVICE = "Denial of Service"
    ELEVATION_OF_PRIVILEGE = "Elevation of Privilege"


class Severity(Enum):
    """Threat severity levels"""
    CRITICAL = 4
    HIGH = 3
    MEDIUM = 2
    LOW = 1


class ComponentType(Enum):
    """System component types"""
    WEB_APP = "Web Application"
    API = "API Endpoint"
    DATABASE = "Database"
    AUTH = "Authentication"
    FILE_STORAGE = "File Storage"
    EXTERNAL_SERVICE = "External Service"
    MESSAGE_QUEUE = "Message Queue"
    CACHE = "Cache Layer"


@dataclass
class SystemComponent:
    """A component in the system architecture"""
    name: str
    type: ComponentType
    file_path: str
    description: str
    entry_points: List[str] = field(default_factory=list)
    data_flows: List[str] = field(default_factory=list)
    trust_boundary: str = "internal"


@dataclass
class Threat:
    """A security threat identified in the system"""
    id: str
    category: ThreatCategory
    name: str
    description: str
    affected_component: str
    attack_vector: str
    severity: Severity
    likelihood: str
    impact: str
    mitigations: List[str]
    controls: List[str] = field(default_factory=list)
    cwe_id: Optional[str] = None


@dataclass
class ThreatFinding:
    """A threat finding from analysis"""
    threat: Threat
    evidence: List[str]
    risk_score: int
    priority: str


class ThreatModeler:
    """STRIDE-based threat modeling tool"""

    SKIP_DIRS = {
        'node_modules', '__pycache__', '.git', '.svn', '.hg',
        'venv', 'env', '.venv', 'dist', 'build', '.tox',
        'vendor', 'third_party', '.idea', '.vscode'
    }

    # Patterns to identify system components
    COMPONENT_PATTERNS = {
        ComponentType.API: [
            r'(?i)@(?:app\.route|router\.|api\.)',
            r'(?i)(?:express|fastapi|flask)\.(?:get|post|put|delete)',
            r'(?i)(?:endpoint|handler|controller)',
        ],
        ComponentType.AUTH: [
            r'(?i)(?:authenticate|login|logout|session|jwt|oauth|token)',
            r'(?i)(?:password|credential|secret|bearer)',
            r'(?i)(?:authorize|permission|role|acl)',
        ],
        ComponentType.DATABASE: [
            r'(?i)(?:mongoose|sequelize|prisma|knex|sqlalchemy)',
            r'(?i)(?:SELECT|INSERT|UPDATE|DELETE).*(?:FROM|INTO)',
            r'(?i)(?:mongodb|postgresql|mysql|redis)',
        ],
        ComponentType.FILE_STORAGE: [
            r'(?i)(?:upload|download|file|attachment)',
            r'(?i)(?:s3|blob|storage|bucket)',
            r'(?i)(?:fs\.|path\.|file_path)',
        ],
        ComponentType.EXTERNAL_SERVICE: [
            r'(?i)(?:http\.get|http\.post|requests\.|fetch\()',
            r'(?i)(?:webhook|callback|integration)',
            r'(?i)(?:api\..*\.com|external)',
        ],
    }

    # STRIDE threat templates
    STRIDE_THREATS = {
        ThreatCategory.SPOOFING: [
            {
                'name': 'Authentication Bypass',
                'description': 'Attacker could bypass authentication to impersonate legitimate users',
                'attack_vector': 'Weak authentication, credential stuffing, session hijacking',
                'severity': Severity.CRITICAL,
                'likelihood': 'High if authentication is weak or missing',
                'impact': 'Full account takeover, unauthorized access to sensitive data',
                'mitigations': ['Implement MFA', 'Use strong session management', 'Validate all auth tokens'],
                'cwe_id': 'CWE-287',
                'patterns': [r'(?i)(?:auth|login|session)', r'(?i)(?:token|jwt|bearer)']
            },
            {
                'name': 'Session Fixation',
                'description': 'Attacker could fix a session ID and hijack user session',
                'attack_vector': 'Setting session ID before authentication',
                'severity': Severity.HIGH,
                'likelihood': 'Medium if session management is weak',
                'impact': 'Session hijacking, impersonation',
                'mitigations': ['Regenerate session ID on login', 'Use secure session configuration'],
                'cwe_id': 'CWE-384',
                'patterns': [r'(?i)session', r'(?i)(?:cookie|sid)']
            },
        ],
        ThreatCategory.TAMPERING: [
            {
                'name': 'SQL Injection',
                'description': 'Attacker could inject malicious SQL to modify database',
                'attack_vector': 'Unsanitized user input in SQL queries',
                'severity': Severity.CRITICAL,
                'likelihood': 'High if using string concatenation in queries',
                'impact': 'Data modification, deletion, or exfiltration',
                'mitigations': ['Use parameterized queries', 'Implement input validation', 'Apply least privilege'],
                'cwe_id': 'CWE-89',
                'patterns': [r'(?i)(?:execute|query|cursor)', r'(?i)(?:SELECT|INSERT|UPDATE)']
            },
            {
                'name': 'Parameter Tampering',
                'description': 'Attacker could modify request parameters to bypass controls',
                'attack_vector': 'Manipulating hidden fields, cookies, or URL parameters',
                'severity': Severity.HIGH,
                'likelihood': 'High for applications relying on client-side validation',
                'impact': 'Price manipulation, privilege escalation, data modification',
                'mitigations': ['Server-side validation', 'Sign sensitive parameters', 'Use integrity checks'],
                'cwe_id': 'CWE-472',
                'patterns': [r'(?i)(?:params|request|body)', r'(?i)(?:price|amount|quantity)']
            },
        ],
        ThreatCategory.REPUDIATION: [
            {
                'name': 'Missing Audit Trail',
                'description': 'System lacks logging to track user actions',
                'attack_vector': 'User denies performing malicious actions',
                'severity': Severity.MEDIUM,
                'likelihood': 'High if logging is not implemented',
                'impact': 'Cannot prove user actions, compliance failures',
                'mitigations': ['Implement comprehensive logging', 'Use tamper-proof logs', 'Log all security events'],
                'cwe_id': 'CWE-778',
                'patterns': [r'(?i)(?:log|audit|trace)', r'(?i)(?:event|action|activity)']
            },
            {
                'name': 'Insufficient Transaction Logging',
                'description': 'Financial or critical transactions not properly logged',
                'attack_vector': 'Deny financial transactions or data changes',
                'severity': Severity.HIGH,
                'likelihood': 'Medium for financial applications',
                'impact': 'Fraud, compliance violations, inability to investigate',
                'mitigations': ['Log all transactions', 'Include timestamps and user IDs', 'Implement non-repudiation'],
                'cwe_id': 'CWE-779',
                'patterns': [r'(?i)(?:transaction|payment|order)', r'(?i)(?:create|update|delete)']
            },
        ],
        ThreatCategory.INFORMATION_DISCLOSURE: [
            {
                'name': 'Sensitive Data Exposure',
                'description': 'Sensitive data could be exposed in logs, errors, or responses',
                'attack_vector': 'Verbose error messages, insecure logging, response data leakage',
                'severity': Severity.HIGH,
                'likelihood': 'High if debug mode is enabled in production',
                'impact': 'PII exposure, credential leakage, compliance violations',
                'mitigations': ['Disable debug mode', 'Sanitize error messages', 'Encrypt sensitive data'],
                'cwe_id': 'CWE-200',
                'patterns': [r'(?i)(?:password|secret|token)', r'(?i)(?:debug|error|exception)']
            },
            {
                'name': 'Path Traversal',
                'description': 'Attacker could access files outside intended directory',
                'attack_vector': 'Using ../ sequences to traverse directories',
                'severity': Severity.HIGH,
                'likelihood': 'Medium if file paths are constructed from user input',
                'impact': 'Access to sensitive files, source code disclosure',
                'mitigations': ['Validate file paths', 'Use whitelisting', 'Sandbox file access'],
                'cwe_id': 'CWE-22',
                'patterns': [r'(?i)(?:file|path|directory)', r'(?i)(?:open|read|download)']
            },
        ],
        ThreatCategory.DENIAL_OF_SERVICE: [
            {
                'name': 'Resource Exhaustion',
                'description': 'Attacker could exhaust system resources',
                'attack_vector': 'Sending large requests, creating many connections, infinite loops',
                'severity': Severity.HIGH,
                'likelihood': 'Medium without rate limiting',
                'impact': 'Service unavailability, system crash',
                'mitigations': ['Implement rate limiting', 'Set resource limits', 'Use input size validation'],
                'cwe_id': 'CWE-400',
                'patterns': [r'(?i)(?:upload|file|request)', r'(?i)(?:size|limit|max)']
            },
            {
                'name': 'ReDoS (Regular Expression DoS)',
                'description': 'Malicious input could cause regex to take exponential time',
                'attack_vector': 'Crafted input exploiting backtracking in regex',
                'severity': Severity.MEDIUM,
                'likelihood': 'Medium if complex regex is used on user input',
                'impact': 'Service slowdown or unavailability',
                'mitigations': ['Audit regex patterns', 'Set regex timeouts', 'Use RE2 library'],
                'cwe_id': 'CWE-1333',
                'patterns': [r're\.|regex|pattern', r'(?i)(?:match|search|replace)']
            },
        ],
        ThreatCategory.ELEVATION_OF_PRIVILEGE: [
            {
                'name': 'Privilege Escalation',
                'description': 'User could gain unauthorized elevated privileges',
                'attack_vector': 'Exploiting IDOR, missing authorization checks, role manipulation',
                'severity': Severity.CRITICAL,
                'likelihood': 'High if authorization checks are client-side only',
                'impact': 'Admin access, full system compromise',
                'mitigations': ['Server-side authorization', 'Principle of least privilege', 'Role-based access control'],
                'cwe_id': 'CWE-269',
                'patterns': [r'(?i)(?:role|admin|privilege)', r'(?i)(?:authorize|permission|access)']
            },
            {
                'name': 'Insecure Direct Object Reference (IDOR)',
                'description': 'User could access objects belonging to other users',
                'attack_vector': 'Modifying object IDs in requests',
                'severity': Severity.HIGH,
                'likelihood': 'High if object ownership is not verified',
                'impact': 'Access to other users\' data, horizontal privilege escalation',
                'mitigations': ['Verify object ownership', 'Use indirect references', 'Implement proper authorization'],
                'cwe_id': 'CWE-639',
                'patterns': [r'(?i)(?:id|user_id|account)', r'(?i)(?:get|fetch|find)']
            },
        ],
    }

    def __init__(self, target_path: str, config: Optional[Dict] = None,
                 verbose: bool = False):
        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)
        self.target_path = Path(target_path)
        self.config = config or {}
        self.verbose = verbose
        self.components: List[SystemComponent] = []
        self.findings: List[ThreatFinding] = []
        self.files_analyzed = 0
        logger.debug("ThreatModeler initialized")

    def run(self) -> Dict:
        """Execute threat modeling analysis"""
        logger.debug("Starting threat modeling execution")
        if self.verbose:
            print(f"Starting threat modeling: {self.target_path}")

        if not self.target_path.exists():
            logger.error(f"Target path does not exist: {self.target_path}")
            raise ValueError(f"Target path does not exist: {self.target_path}")

        # Discover system components
        self._discover_components()

        # Identify threats using STRIDE
        self._identify_threats()

        return self._generate_results()

    def _discover_components(self):
        """Discover system components from codebase"""
        logger.debug("Discovering system components")
        if self.target_path.is_file():
            self._analyze_file(self.target_path)
        else:
            for root, dirs, files in os.walk(self.target_path):
                dirs[:] = [d for d in dirs if d not in self.SKIP_DIRS]
                for file_name in files:
                    file_path = Path(root) / file_name
                    self._analyze_file(file_path)

    def _analyze_file(self, file_path: Path):
        """Analyze file for system components"""
        try:
            # Skip non-code files
            if file_path.suffix.lower() not in {'.py', '.js', '.ts', '.tsx', '.jsx', '.go', '.java', '.rb', '.php'}:
                return

            if self._is_binary(file_path):
                return

            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            self.files_analyzed += 1

            # Identify component types
            for comp_type, patterns in self.COMPONENT_PATTERNS.items():
                for pattern in patterns:
                    if re.search(pattern, content):
                        # Check if we already have this component type for this file
                        existing = next(
                            (c for c in self.components if c.file_path == str(file_path) and c.type == comp_type),
                            None
                        )
                        if not existing:
                            entry_points = self._find_entry_points(content, comp_type)
                            self.components.append(SystemComponent(
                                name=file_path.stem,
                                type=comp_type,
                                file_path=str(file_path),
                                description=f"{comp_type.value} component",
                                entry_points=entry_points
                            ))
                        break

        except Exception as e:
            logger.warning(f"Error analyzing {file_path}: {e}")
            if self.verbose:
                print(f"  Error analyzing {file_path}: {e}")

    def _find_entry_points(self, content: str, comp_type: ComponentType) -> List[str]:
        """Find entry points in the code"""
        entry_points = []

        if comp_type == ComponentType.API:
            # Find route definitions
            route_patterns = [
                r'@(?:app|router)\.(?:route|get|post|put|delete|patch)\s*\([\'"]([^\'"]+)',
                r'\.(?:get|post|put|delete)\s*\([\'"]([^\'"]+)',
            ]
            for pattern in route_patterns:
                for match in re.finditer(pattern, content):
                    entry_points.append(match.group(1))

        return entry_points[:10]  # Limit to 10

    def _is_binary(self, file_path: Path) -> bool:
        """Check if file is binary"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(512)
                return b'\x00' in chunk
        except Exception:
            return True

    def _identify_threats(self):
        """Identify threats using STRIDE methodology"""
        logger.debug("Identifying threats using STRIDE methodology")
        for category, threats in self.STRIDE_THREATS.items():
            for threat_template in threats:
                evidence = []

                # Check each component for threat relevance
                for component in self.components:
                    try:
                        with open(component.file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()

                        # Check threat patterns against content
                        for pattern in threat_template.get('patterns', []):
                            if re.search(pattern, content):
                                evidence.append(f"{component.file_path}: Pattern match - {pattern}")
                    except Exception:
                        continue

                # If evidence found, create a finding
                if evidence:
                    threat = Threat(
                        id=f"{category.name[:3]}-{len(self.findings)+1:03d}",
                        category=category,
                        name=threat_template['name'],
                        description=threat_template['description'],
                        affected_component=", ".join([c.name for c in self.components if any(
                            c.file_path in e for e in evidence
                        )])[:100] or "Multiple components",
                        attack_vector=threat_template['attack_vector'],
                        severity=threat_template['severity'],
                        likelihood=threat_template['likelihood'],
                        impact=threat_template['impact'],
                        mitigations=threat_template['mitigations'],
                        cwe_id=threat_template.get('cwe_id')
                    )

                    risk_score = self._calculate_risk_score(threat, len(evidence))
                    priority = self._determine_priority(risk_score)

                    self.findings.append(ThreatFinding(
                        threat=threat,
                        evidence=evidence[:5],  # Limit evidence
                        risk_score=risk_score,
                        priority=priority
                    ))

    def _calculate_risk_score(self, threat: Threat, evidence_count: int) -> int:
        """Calculate risk score (1-100)"""
        # Base score from severity
        base_score = threat.severity.value * 20  # 20-80

        # Adjust for evidence count
        evidence_factor = min(evidence_count / 5, 1.0)  # Max 1.0

        # Calculate final score
        score = int(base_score + (20 * evidence_factor))
        return min(100, score)

    def _determine_priority(self, risk_score: int) -> str:
        """Determine priority based on risk score"""
        if risk_score >= 80:
            return "P1 - Critical"
        elif risk_score >= 60:
            return "P2 - High"
        elif risk_score >= 40:
            return "P3 - Medium"
        else:
            return "P4 - Low"

    def _generate_results(self) -> Dict:
        """Generate comprehensive threat model results"""
        logger.debug("Generating threat modeling results")
        if not self.findings:
            logger.warning("No threats identified in analysis")
        # Count by category
        category_counts = {cat.value: 0 for cat in ThreatCategory}
        for finding in self.findings:
            category_counts[finding.threat.category.value] += 1

        # Count by severity
        severity_counts = {s.name: 0 for s in Severity}
        for finding in self.findings:
            severity_counts[finding.threat.severity.name] += 1

        # Calculate overall risk
        overall_risk = self._calculate_overall_risk()

        # Generate attack surface summary
        attack_surface = self._generate_attack_surface()

        results = {
            'status': 'completed',
            'timestamp': datetime.now().isoformat(),
            'target': str(self.target_path),
            'analysis_stats': {
                'files_analyzed': self.files_analyzed,
                'components_found': len(self.components),
                'threats_identified': len(self.findings)
            },
            'summary': {
                'overall_risk': overall_risk,
                'critical': severity_counts['CRITICAL'],
                'high': severity_counts['HIGH'],
                'medium': severity_counts['MEDIUM'],
                'low': severity_counts['LOW']
            },
            'stride_analysis': category_counts,
            'attack_surface': attack_surface,
            'components': [self._component_to_dict(c) for c in self.components],
            'threats': [self._finding_to_dict(f) for f in sorted(
                self.findings, key=lambda x: x.risk_score, reverse=True
            )],
            'recommendations': self._generate_recommendations()
        }

        return results

    def _calculate_overall_risk(self) -> str:
        """Calculate overall risk rating"""
        if not self.findings:
            return "Low"

        critical = sum(1 for f in self.findings if f.threat.severity == Severity.CRITICAL)
        high = sum(1 for f in self.findings if f.threat.severity == Severity.HIGH)

        if critical >= 2 or (critical >= 1 and high >= 3):
            return "Critical"
        elif critical >= 1 or high >= 3:
            return "High"
        elif high >= 1:
            return "Medium"
        else:
            return "Low"

    def _generate_attack_surface(self) -> Dict:
        """Generate attack surface analysis"""
        entry_points = []
        data_stores = []
        external_deps = []

        for comp in self.components:
            if comp.type == ComponentType.API:
                entry_points.extend(comp.entry_points)
            elif comp.type == ComponentType.DATABASE:
                data_stores.append(comp.name)
            elif comp.type == ComponentType.EXTERNAL_SERVICE:
                external_deps.append(comp.name)

        return {
            'entry_points_count': len(entry_points),
            'entry_points': entry_points[:20],
            'data_stores': list(set(data_stores)),
            'external_dependencies': list(set(external_deps)),
            'component_types': {ct.value: sum(1 for c in self.components if c.type == ct)
                               for ct in ComponentType if any(c.type == ct for c in self.components)}
        }

    def _component_to_dict(self, comp: SystemComponent) -> Dict:
        """Convert component to dictionary"""
        return {
            'name': comp.name,
            'type': comp.type.value,
            'file_path': comp.file_path,
            'description': comp.description,
            'entry_points': comp.entry_points
        }

    def _finding_to_dict(self, finding: ThreatFinding) -> Dict:
        """Convert finding to dictionary"""
        return {
            'id': finding.threat.id,
            'category': finding.threat.category.value,
            'name': finding.threat.name,
            'description': finding.threat.description,
            'affected_component': finding.threat.affected_component,
            'attack_vector': finding.threat.attack_vector,
            'severity': finding.threat.severity.name,
            'likelihood': finding.threat.likelihood,
            'impact': finding.threat.impact,
            'risk_score': finding.risk_score,
            'priority': finding.priority,
            'mitigations': finding.threat.mitigations,
            'cwe_id': finding.threat.cwe_id,
            'evidence': finding.evidence
        }

    def _generate_recommendations(self) -> List[str]:
        """Generate prioritized recommendations"""
        recommendations = []

        critical = [f for f in self.findings if f.threat.severity == Severity.CRITICAL]
        high = [f for f in self.findings if f.threat.severity == Severity.HIGH]

        if critical:
            recommendations.append(
                f"CRITICAL: Address {len(critical)} critical threats immediately. "
                "These represent significant security risks."
            )
            for finding in critical[:3]:
                recommendations.append(f"  - {finding.threat.name}: {finding.threat.mitigations[0]}")

        if high:
            recommendations.append(
                f"HIGH: {len(high)} high-severity threats require attention in the near term."
            )

        # Add category-specific recommendations
        category_counts = {}
        for f in self.findings:
            cat = f.threat.category
            category_counts[cat] = category_counts.get(cat, 0) + 1

        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            if count >= 2:
                recommendations.append(
                    f"{cat.value}: {count} threats identified. Consider implementing controls for this category."
                )

        if not recommendations:
            recommendations.append(
                "Low risk profile. Continue monitoring and implementing security best practices."
            )

        return recommendations


class OutputFormatter:
    """Format threat model results for output"""

    @staticmethod
    def format_text(results: Dict, verbose: bool = False) -> str:
        """Format results as human-readable text"""
        output = []
        output.append("=" * 80)
        output.append("THREAT MODEL REPORT")
        output.append("=" * 80)
        output.append(f"Timestamp: {results['timestamp']}")
        output.append(f"Target: {results['target']}")
        output.append(f"Files Analyzed: {results['analysis_stats']['files_analyzed']}")
        output.append(f"Components Found: {results['analysis_stats']['components_found']}")
        output.append("")

        summary = results['summary']
        output.append("RISK SUMMARY")
        output.append("-" * 80)
        output.append(f"Overall Risk: {summary['overall_risk']}")
        output.append(f"Total Threats: {results['analysis_stats']['threats_identified']}")
        output.append(f"  Critical: {summary['critical']}")
        output.append(f"  High: {summary['high']}")
        output.append(f"  Medium: {summary['medium']}")
        output.append(f"  Low: {summary['low']}")
        output.append("")

        output.append("STRIDE ANALYSIS")
        output.append("-" * 80)
        for category, count in results['stride_analysis'].items():
            if count > 0:
                output.append(f"  {category}: {count}")
        output.append("")

        output.append("ATTACK SURFACE")
        output.append("-" * 80)
        attack_surface = results['attack_surface']
        output.append(f"  Entry Points: {attack_surface['entry_points_count']}")
        output.append(f"  Data Stores: {len(attack_surface['data_stores'])}")
        output.append(f"  External Dependencies: {len(attack_surface['external_dependencies'])}")
        output.append("")

        if results['threats']:
            output.append("IDENTIFIED THREATS")
            output.append("-" * 80)
            for threat in results['threats'][:20]:
                output.append(f"[{threat['severity']}] {threat['id']}: {threat['name']}")
                output.append(f"  Category: {threat['category']}")
                output.append(f"  Risk Score: {threat['risk_score']}/100 ({threat['priority']})")
                output.append(f"  Description: {threat['description']}")
                output.append(f"  Mitigations: {'; '.join(threat['mitigations'][:2])}")
                if threat.get('cwe_id'):
                    output.append(f"  CWE: {threat['cwe_id']}")
                output.append("")

        output.append("RECOMMENDATIONS")
        output.append("-" * 80)
        for i, rec in enumerate(results['recommendations'], 1):
            output.append(f"{i}. {rec}")

        output.append("=" * 80)
        return "\n".join(output)

    @staticmethod
    def format_json(results: Dict) -> str:
        """Format results as JSON"""
        return json.dumps(results, indent=2)

    @staticmethod
    def format_csv(results: Dict) -> str:
        """Format results as CSV"""
        output = StringIO()
        writer = csv.writer(output)
        writer.writerow(['id', 'category', 'name', 'severity', 'risk_score', 'priority',
                        'description', 'attack_vector', 'cwe_id'])
        for threat in results['threats']:
            writer.writerow([
                threat['id'], threat['category'], threat['name'], threat['severity'],
                threat['risk_score'], threat['priority'], threat['description'],
                threat['attack_vector'], threat.get('cwe_id', '')
            ])
        return output.getvalue()


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Threat Modeler - STRIDE Security Threat Analysis Tool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze a project directory
  %(prog)s --input /path/to/project

  # Generate JSON report
  %(prog)s --input /path/to/project --output json --file threats.json

  # Verbose output with progress
  %(prog)s --input /path/to/project --verbose

STRIDE Categories:
  S - Spoofing Identity
  T - Tampering with Data
  R - Repudiation
  I - Information Disclosure
  D - Denial of Service
  E - Elevation of Privilege
        """
    )

    parser.add_argument('--input', '-i', required=True, help='File or directory to analyze')
    parser.add_argument('--output', '-o', choices=['text', 'json', 'csv'], default='text',
                       help='Output format (default: text)')
    parser.add_argument('--config', '-c', help='Configuration file path (JSON)')
    parser.add_argument('--file', '-f', help='Write output to file')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')

    parser.add_argument(
        '--version',
        action='version',
        version='%(prog)s 1.0.0'
    )

    args = parser.parse_args()

    # Validate input
    if not os.path.exists(args.input):
        print(f"Error: Input path does not exist: {args.input}", file=sys.stderr)
        sys.exit(1)

    # Load config if provided
    config = {}
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = json.load(f)

    try:
        # Run threat modeling
        modeler = ThreatModeler(
            target_path=args.input,
            config=config,
            verbose=args.verbose
        )
        results = modeler.run()

        # Format output
        formatter = OutputFormatter()
        if args.output == 'json':
            output_text = formatter.format_json(results)
        elif args.output == 'csv':
            output_text = formatter.format_csv(results)
        else:
            output_text = formatter.format_text(results, verbose=args.verbose)

        # Write output
        if args.file:
            with open(args.file, 'w') as f:
                f.write(output_text)
            if args.verbose:
                print(f"\nReport saved to: {args.file}")
        else:
            print(output_text)

        # Exit code based on risk level
        if results['summary']['overall_risk'] == 'Critical':
            sys.exit(2)
        elif results['summary']['overall_risk'] == 'High':
            sys.exit(1)
        else:
            sys.exit(0)

    except KeyboardInterrupt:
        print("\nAnalysis interrupted", file=sys.stderr)
        sys.exit(130)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
