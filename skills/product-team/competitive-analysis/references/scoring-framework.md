# Competitive Scoring Framework

## Overview

This framework defines the scoring criteria, rubrics, and methodology for evaluating competitive products against the claude-skills library. Use this reference when conducting competitive analysis to ensure consistent, objective scoring.

---

## Scoring Dimensions

### Dimension 1: Documentation Completeness (20%)

**What to Evaluate**: Quality, depth, and usability of documentation

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | Complete YAML metadata, all sections present, 3+ examples, clear navigation, comprehensive API docs |
| â­â­â­â­ (4) | Most metadata present, key sections covered, 2+ examples, good structure |
| â­â­â­ (3) | Basic metadata, main sections present, at least 1 example, readable |
| â­â­ (2) | Incomplete metadata, missing sections, no examples, basic structure |
| â­ (1) | Minimal or no documentation, unclear structure, unusable |

**Checklist**:
- [ ] YAML frontmatter with required fields
- [ ] Overview/description section
- [ ] Usage examples (3+ for full score)
- [ ] API/CLI documentation
- [ ] Error handling documented
- [ ] Integration guides
- [ ] Version history

---

### Dimension 2: Tool/Script Quality (20%)

**What to Evaluate**: Quality of Python tools, CLI support, and automation

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | Multiple tools, full CLI with --help, error handling, JSON output, tests, zero dependencies |
| â­â­â­â­ (4) | 2+ tools, CLI support, good error handling, multiple output formats |
| â­â­â­ (3) | 1-2 tools, basic CLI, some error handling, single output format |
| â­â­ (2) | Tools present but limited, minimal CLI, poor error handling |
| â­ (1) | No tools or broken/unusable scripts |

**Checklist**:
- [ ] CLI interface with argparse
- [ ] --help flag support
- [ ] Error handling with useful messages
- [ ] Multiple output formats (json, markdown, console)
- [ ] Zero external dependencies
- [ ] Input validation
- [ ] Tests or validation scripts

---

### Dimension 3: Workflow Coverage (15%)

**What to Evaluate**: Number and depth of documented workflows

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | 4+ complete workflows, clear triggers, detailed steps, deliverables defined |
| â­â­â­â­ (4) | 3-4 workflows, good detail, most steps documented |
| â­â­â­ (3) | 2-3 workflows, moderate detail, basic steps |
| â­â­ (2) | 1-2 workflows, limited detail |
| â­ (1) | No documented workflows or unclear processes |

**Workflow Quality Criteria**:
- Purpose clearly stated
- Trigger conditions defined
- Step-by-step process
- Tools/resources referenced
- Expected deliverables
- Example outputs

---

### Dimension 4: Architecture (15%)

**What to Evaluate**: Modularity, portability, dependency management

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | Zero dependencies, fully modular, works standalone, clean separation of concerns |
| â­â­â­â­ (4) | Minimal dependencies, modular design, portable |
| â­â­â­ (3) | Some dependencies, mostly modular, some portability issues |
| â­â­ (2) | Heavy dependencies, tightly coupled, difficult to extract |
| â­ (1) | Monolithic, many dependencies, not portable |

**Checklist**:
- [ ] Zero external pip dependencies
- [ ] Self-contained skill packages
- [ ] Clean folder structure
- [ ] Relative path usage
- [ ] No hardcoded values
- [ ] Platform-agnostic code

---

### Dimension 5: Automation (15%)

**What to Evaluate**: Auto-generation, validation, CI/CD integration

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | Builder tools, full validation, 100% pass rate, automated updates |
| â­â­â­â­ (4) | Builder tools, validation checks, high pass rate |
| â­â­â­ (3) | Some automation, basic validation |
| â­â­ (2) | Limited automation, manual processes |
| â­ (1) | No automation, all manual |

**Checklist**:
- [ ] Builder/scaffolding tools
- [ ] Validation scripts
- [ ] Documented validation criteria
- [ ] Pass/fail reporting
- [ ] Automated catalog updates
- [ ] CI/CD integration

---

### Dimension 6: Reference Depth (15%)

**What to Evaluate**: Knowledge bases, templates, supporting materials

| Score | Criteria |
|-------|----------|
| â­â­â­â­â­ (5) | Comprehensive references, multiple templates, expert knowledge bases, examples for all use cases |
| â­â­â­â­ (4) | Good references, useful templates, solid knowledge base |
| â­â­â­ (3) | Some references, basic templates |
| â­â­ (2) | Limited references, few templates |
| â­ (1) | No references or templates |

**Checklist**:
- [ ] Reference markdown files
- [ ] User-facing templates
- [ ] Best practices documentation
- [ ] Example outputs
- [ ] Integration guides
- [ ] FAQ or troubleshooting

---

## Scoring Calculation

### Per-Item Score

```
Item Score = Î£ (Dimension Score Ã— Weight)

Where:
- Documentation: Score Ã— 0.20
- Tool Quality:  Score Ã— 0.20
- Workflows:     Score Ã— 0.15
- Architecture:  Score Ã— 0.15
- Automation:    Score Ã— 0.15
- References:    Score Ã— 0.15
```

**Example**:
```
Skill: competitor-skill-a
- Documentation: 4 Ã— 0.20 = 0.80
- Tool Quality:  3 Ã— 0.20 = 0.60
- Workflows:     5 Ã— 0.15 = 0.75
- Architecture:  4 Ã— 0.15 = 0.60
- Automation:    2 Ã— 0.15 = 0.30
- References:    3 Ã— 0.15 = 0.45
-----------------------------
Total: 3.50 / 5.00 (70%)
```

### Aggregate Score

```
Overall Score = Î£ (All Item Scores) / Number of Items
```

---

## Comparison Outcomes

### Winner Determination

| Symbol | Outcome | Criteria |
|--------|---------|----------|
| ðŸŸ¢ | Better | Our score > Competitor score by â‰¥0.5 |
| âœ… | Same | Scores within 0.5 of each other |
| ðŸŸ¡ | Different | Different approaches, neither objectively better |
| âŒ | Behind | Competitor score > Our score by â‰¥0.5 |

### Overall Assessment

| Assessment | Criteria |
|------------|----------|
| **SIGNIFICANTLY AHEAD** | 70%+ features ðŸŸ¢ Better |
| **AHEAD** | 50-69% features ðŸŸ¢ Better, <20% âŒ Behind |
| **COMPETITIVE** | 40-60% features âœ… Same |
| **BEHIND** | 30%+ features âŒ Behind |
| **SIGNIFICANTLY BEHIND** | 50%+ features âŒ Behind |

---

## Confidence Levels

| Level | Criteria |
|-------|----------|
| **HIGH** | Full access to competitor code, all dimensions evaluated |
| **MEDIUM** | Partial access, most dimensions evaluated |
| **LOW** | Limited access, some dimensions estimated |

---

## Scoring Tips

### Do's

1. **Be Objective**: Apply same criteria to both sides
2. **Document Evidence**: Note specific examples for each score
3. **Use Checklists**: Ensure comprehensive evaluation
4. **Compare Apples to Apples**: Match similar items

### Don'ts

1. **Don't Assume**: Score based on evidence, not assumptions
2. **Don't Over-Weight**: Stick to defined percentages
3. **Don't Rush**: Take time for thorough analysis
4. **Don't Bias**: Acknowledge competitor strengths honestly

---

## Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SCORING QUICK REFERENCE               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Dimension          â”‚ Weight â”‚ Max Score        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Documentation      â”‚  20%   â”‚  5 (1.00 pts)   â”‚
â”‚  Tool Quality       â”‚  20%   â”‚  5 (1.00 pts)   â”‚
â”‚  Workflows          â”‚  15%   â”‚  5 (0.75 pts)   â”‚
â”‚  Architecture       â”‚  15%   â”‚  5 (0.75 pts)   â”‚
â”‚  Automation         â”‚  15%   â”‚  5 (0.75 pts)   â”‚
â”‚  References         â”‚  15%   â”‚  5 (0.75 pts)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Maximum Total Score: 5.00 (100%)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸŸ¢ Better: >0.5 ahead  â”‚  âœ… Same: Â±0.5       â”‚
â”‚  ðŸŸ¡ Different approach  â”‚  âŒ Behind: >0.5     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Last Updated**: November 27, 2025
